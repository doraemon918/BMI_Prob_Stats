{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods\n",
    "\n",
    "##### For a better and more in depth explanation of Resampling methods, refer to the free online book \"An Introduction to Statistical Learning with Applications in R by James et. al. [http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf)\n",
    "\n",
    "Most of you are familiar with the terms *training* and *testing* sets which refer to splitting the data into two subsets that will be used to \"develop\" the model - the learning step - and a to test the model -the evaluation step. \n",
    "\n",
    "For example: if we want to understand how much variation we can expect from a linear regression, using resampling methods we can draw multiple random samples from the training data, fit a linear regression to each sample, and measure how much variation there is across subsets.\n",
    "\n",
    "We have already seen few aspects of resampling methods when we used boostrapping to obtain confidence intervals around a mean. Boostrapping is an efficient and powerful method to increase the level of accuracy of a parameter estimate.\n",
    "\n",
    "Another resampling technique that will be the focus for this lecture is Cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "Cross-validation can be used to evaluate the performance of a statistical test, and to estimate a measurement of error from this statistical analysis more accurately.\n",
    "\n",
    "The basic concept as mentioned before is to partition the data into subsets that can be used to measure the error differences across subsets and to evaluate the performance using a hold out set or test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, one of the most frequently used error estimators for regression models is the MSE(mean squared error), which is a measurement of quality of the fit for a particular model.\n",
    "\n",
    "The idea is to measure how close is the predicted response for each observation to the true response value for that observation.\n",
    "\n",
    "$$ MSE = \\frac {1}{n} \\sum _{i = 1}^{n} (y_i - \\hat {f}(x_i))^2  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\hat{f}_i$ is the prediction of $\\hat{f}$ on each observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = read.table(\"CH01PR27.dat\")\n",
    "\n",
    "#Check that is read properly. Also check which column is Y.\n",
    "Data\n",
    "\n",
    "#Re-name the columns of the Data\n",
    "#X=age, Y=muscle mass\n",
    "names(Data) = c(\"Y\",\"X\")\n",
    "\n",
    "#a) Fit the regression model. Obtain the estimates. Plot the estimated regression function and the data\n",
    "Muscle = lm(Y ~ X, data = Data)\n",
    "\n",
    "#Print summary of estimated coefficients\n",
    "summary(Muscle)\n",
    "\n",
    "#Plot data and regression line\n",
    "plot(Data$X, Data$Y, main=\"Muscle\",\n",
    "\t\txlab=\"Age (years)\", ylab=\"Measure of Muscle Mass\", pch=19)\n",
    "lines(Data$X, Muscle$fitted.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Find mse\n",
    "anova(Muscle)\n",
    "8.344^2 ##From the Residual standard error in the summary of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This MSE is the overall MSE for the model, however, we want to evaluate how well the method works on an untested set (partition) not used to train the model.\n",
    "\n",
    "One option would also be to calculate the MSE from the model used for training and ideally this values is very small giving an indication of how good the model is for that partition. However, we don't really care to know how well the model predict what we already know, or to calculate a training MSE. What we want to do is to evaluate the model to a subset that has not been used to evaluate the model.\n",
    "\n",
    "We want to chose a method that gives the lowest MSE, as opposed to the lowest training MSE, if we have a large number of test observations we can calculate the average MSE and select a model with the lowest test MSE.\n",
    "\n",
    "We are going to follow an example directed to understand the use of tunable parameters to estimate the accuracy of the model.\n",
    "\n",
    "First, we are going to simulate 100 training sets of size 50 from a polynomial regression model, and we will run a polynomial using a cubic spline models with degrees of freedom from 1 to 30 (the tunable parameter). \n",
    "\n",
    "### Remember, in this first exercise we are not splitting data from the dataset  to produced the training data, what we are doing is to get data obtained from the same polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training and test samples\n",
    "seed <- 1809\n",
    "set.seed(seed)\n",
    " \n",
    "##Write function to generate x and y dataframe using polynomial x variable taken from a uniform distribution.\n",
    "gen_data <- function(n, beta, sigma_eps) {\n",
    "    eps <- rnorm(n, 0, sigma_eps)\n",
    "    x <- sort(runif(n, 0, 100))\n",
    "    X <- cbind(1, poly(x, degree = (length(beta) - 1), raw = TRUE))\n",
    "    y <- as.numeric(X %*% beta + eps)\n",
    "    \n",
    "    return(data.frame(x = x, y = y))\n",
    "}\n",
    " \n",
    "# Fit the models\n",
    "require(splines)\n",
    " \n",
    "n_rep <- 100\n",
    "n_df <- 30\n",
    "df <- 1:n_df\n",
    "beta <- c(5, -0.1, 0.004, -3e-05) ##These are the different values of beta that will help fit the model to a polynomial\n",
    "n_train <- 50\n",
    "n_test <- 10000\n",
    "sigma_eps <- 0.5\n",
    " \n",
    "xy <- res <- list()\n",
    "xy_test <- gen_data(n_test, beta, sigma_eps)\n",
    "for (i in 1:n_rep) {\n",
    "    xy[[i]] <- gen_data(n_train, beta, sigma_eps)\n",
    "    x <- xy[[i]][, \"x\"]\n",
    "    y <- xy[[i]][, \"y\"]\n",
    "    res[[i]] <- apply(t(df), 2, function(degf) lm(y ~ ns(x, df = degf))) ##Generate a Basis Matrix for \n",
    "        #Natural Cubic Splines from 1 to 30 degrees of freedom\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(xy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now we have our test set and 100 training sets, and that we produced polynomial regressions using different values of splines for the model. Lets visualize how we did on the extreme splines (1,4, and 25) for one of the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "x <- xy[[1]]$x\n",
    "X <- cbind(1, poly(x, degree = (length(beta) - 1), raw = TRUE))\n",
    "y <- xy[[1]]$y\n",
    "plot(y ~ x, col = \"gray\", lwd = 2)\n",
    "lines(x, X %*% beta, lwd = 3, col = \"black\")\n",
    "lines(x, fitted(res[[1]][[1]]), lwd = 3, col = \"palegreen3\")\n",
    "lines(x, fitted(res[[1]][[4]]), lwd = 3, col = \"darkorange\")\n",
    "lines(x, fitted(res[[1]][[25]]), lwd = 3, col = \"steelblue\")\n",
    "legend(x = \"topleft\", legend = c(\"True function\", \"Linear fit (df = 1)\", \"Best model (df = 4)\", \n",
    "    \"Overfitted model (df = 25)\"), lwd = rep(3, 4), col = c(\"black\", \"palegreen3\", \n",
    "    \"darkorange\", \"steelblue\"), text.width = 32, cex = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate for each training set and model the MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the training and test errors for each model\n",
    "pred <- list()\n",
    "mse <- te <- matrix(NA, nrow = n_df, ncol = n_rep)\n",
    "for (i in 1:n_rep) {\n",
    "    mse[, i] <- sapply(res[[i]], function(obj) deviance(obj)/nobs(obj))\n",
    "    pred[[i]] <- mapply(function(obj, degf) predict(obj, data.frame(x = xy_test$x)), \n",
    "        res[[i]], df)\n",
    "    te[, i] <- sapply(as.list(data.frame(pred[[i]])), function(y_hat) mean((xy_test$y - \n",
    "        y_hat)^2))\n",
    "}\n",
    " \n",
    "# Compute the average training and test errors\n",
    "av_mse <- rowMeans(mse)\n",
    "av_te <- rowMeans(te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors\n",
    "plot(df, av_mse, type = \"l\", lwd = 2, col = gray(0.4), ylab = \"Prediction error\", \n",
    "    xlab = \"Flexibilty (spline's degrees of freedom [log scaled])\", ylim = c(0, \n",
    "        1), log = \"x\")\n",
    "abline(h = sigma_eps, lty = 2, lwd = 0.5)\n",
    "for (i in 1:n_rep) {\n",
    "    lines(df, te[, i], col = \"lightpink\")\n",
    "}\n",
    "for (i in 1:n_rep) {\n",
    "    lines(df, mse[, i], col = gray(0.8))\n",
    "}\n",
    "lines(df, av_mse, lwd = 2, col = gray(0.4))\n",
    "lines(df, av_te, lwd = 2, col = \"darkred\")\n",
    "points(df[1], av_mse[1], col = \"palegreen3\", pch = 17, cex = 1.5)\n",
    "points(df[1], av_te[1], col = \"palegreen3\", pch = 17, cex = 1.5)\n",
    "points(df[which.min(av_te)], av_mse[which.min(av_te)], col = \"darkorange\", pch = 16, \n",
    "    cex = 1.5)\n",
    "points(df[which.min(av_te)], av_te[which.min(av_te)], col = \"darkorange\", pch = 16, \n",
    "    cex = 1.5)\n",
    "points(df[25], av_mse[25], col = \"steelblue\", pch = 15, cex = 1.5)\n",
    "points(df[25], av_te[25], col = \"steelblue\", pch = 15, cex = 1.5)\n",
    "legend(x = \"top\", legend = c(\"Training error\", \"Test error\"), lwd = rep(2, 2), \n",
    "    col = c(gray(0.4), \"darkred\"), text.width = 0.3, cex = 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that:\n",
    "\n",
    "1. the training errors decrease monotonically as the model gets more complicated (and less smooth).\n",
    "\n",
    "2. The test error initially decreases, and starts increasing again. The change point occurs in correspondence of the orange model, that is, the model that provides a good compromise between bias and variance. \n",
    "\n",
    "3. The reason why the test error starts increasing for degrees of freedom larger than 3 or 4 is the so called overfitting problem. Overfitting is the tendency of a model to adapt too well to the training data, at the expense of generalization to previously unseen data points. In other words, an overfitted model fits the noise in the data rather than the actual underlying relationships among the variables. Overfitting usually occurs when a model is unnecessarily complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to effectively evaluate the model accuracy we then use Cross-Validation\n",
    "\n",
    "# K-fold Cross validation\n",
    "\n",
    "In this approach we randomly split the observations into k groups, or approximately the same size. The first fold or group is treated as the validation set, and the method is fit on the remaining k-1 methods. On each fold we compute the respective MSE and the final MSE value is averaged and compared with the average of the test set from .model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(seed)\n",
    " \n",
    "n_train <- 100\n",
    "xy <- gen_data(n_train, beta, sigma_eps)\n",
    "x <- xy$x\n",
    "y <- xy$y\n",
    " \n",
    "fitted_models <- apply(t(df), 2, function(degf) lm(y ~ ns(x, df = degf)))\n",
    "mse <- sapply(fitted_models, function(obj) deviance(obj)/nobs(obj))\n",
    " \n",
    "n_test <- 10000\n",
    "xy_test <- gen_data(n_test, beta, sigma_eps)\n",
    "pred <- mapply(function(obj, degf) predict(obj, data.frame(x = xy_test$x)), \n",
    "    fitted_models, df)\n",
    "te <- sapply(as.list(data.frame(pred)), function(y_hat) mean((xy_test$y - y_hat)^2))\n",
    "\n",
    "    \n",
    "n_folds <- 10\n",
    "folds_i <- sample(rep(1:n_folds, length.out = n_train))\n",
    "cv_tmp <- matrix(NA, nrow = n_folds, ncol = length(df))\n",
    "for (k in 1:n_folds) {\n",
    "    test_i <- which(folds_i == k)\n",
    "    train_xy <- xy[-test_i, ]\n",
    "    test_xy <- xy[test_i, ]\n",
    "    x <- train_xy$x\n",
    "    y <- train_xy$y\n",
    "    fitted_models <- apply(t(df), 2, function(degf) lm(y ~ ns(x, df = degf)))\n",
    "    x <- test_xy$x\n",
    "    y <- test_xy$y\n",
    "    pred <- mapply(function(obj, degf) predict(obj, data.frame(ns(x, df = degf))), \n",
    "        fitted_models, df)\n",
    "    cv_tmp[k, ] <- sapply(as.list(data.frame(pred)), function(y_hat) mean((y - \n",
    "        y_hat)^2))\n",
    "}\n",
    "cv <- colMeans(cv_tmp)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "require(Hmisc)\n",
    " \n",
    "plot(df, mse, type = \"l\", lwd = 2, col = gray(0.4), ylab = \"Prediction error\", \n",
    "    xlab = \"Flexibilty (spline's degrees of freedom [log scaled])\", main = paste0(n_folds, \n",
    "        \"-fold Cross-Validation\"), ylim = c(0.1, 0.8), log = \"x\")\n",
    "lines(df, te, lwd = 2, col = \"darkred\", lty = 2)\n",
    "cv_sd <- apply(cv_tmp, 2, sd)/sqrt(n_folds)\n",
    "errbar(df, cv, cv + cv_sd, cv - cv_sd, add = TRUE, col = \"steelblue2\", pch = 19, \n",
    "    lwd = 0.5)\n",
    "lines(df, cv, lwd = 2, col = \"steelblue2\")\n",
    "points(df, cv, col = \"steelblue2\", pch = 19)\n",
    "legend(x = \"topright\", legend = c(\"Training error\", \"Test error\", \"Cross-validation error\"), \n",
    "    lty = c(1, 2, 1), lwd = rep(2, 3), col = c(gray(0.4), \"darkred\", \"steelblue2\"), \n",
    "    text.width = 0.4, cex = 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work through a more concrete example continue the tutorial from [http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r/](http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r/). to the section called:\n",
    "\n",
    "Doing Cross-Validation With R: the caret Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
